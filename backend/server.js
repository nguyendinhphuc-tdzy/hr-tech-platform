/* FILE: backend/server.js (Phi√™n b·∫£n Stable - C·ªë ƒë·ªãnh Model) */
require('dotenv').config();
const express = require('express');
const cors = require('cors');
const { Pool } = require('pg');
const multer = require('multer');
const fs = require('fs'); 
const csv = require('csv-parser');
const mammoth = require('mammoth'); 
const pdf = require('pdf-parse'); 
const { GoogleGenerativeAI } = require("@google/generative-ai");

const app = express();
app.use(cors());
app.use(express.json());

// --- C·∫§U H√åNH C·ªê ƒê·ªäNH (KH√îNG T·ª∞ D√í N·ªÆA) ---
// Model n√†y ·ªïn ƒë·ªãnh nh·∫•t v√† mi·ªÖn ph√≠
const MODEL_NAME = "gemini-1.5-flash"; 

const storage = multer.memoryStorage();
const upload = multer({ storage: storage });

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: { rejectUnauthorized: false }
});

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// ==========================================
// C√ÅC H√ÄM H·ªñ TR·ª¢
// ==========================================

async function readPdfBuffer(buffer) {
    try {
        const data = await pdf(buffer);
        return data.text;
    } catch (err) { return ""; }
}

function chunkText(text, chunkSize = 1000) {
    const chunks = [];
    let currentChunk = "";
    const sentences = text.split(/(?<=[.?!])\s+/);
    for (const sentence of sentences) {
        if ((currentChunk + sentence).length > chunkSize) {
            chunks.push(currentChunk);
            currentChunk = sentence;
        } else { currentChunk += " " + sentence; }
    }
    if (currentChunk) chunks.push(currentChunk);
    return chunks;
}

async function createEmbedding(text) {
    // Model embedding ri√™ng bi·ªát, kh√¥ng li√™n quan ƒë·∫øn Flash/Pro
    const model = genAI.getGenerativeModel({ model: "text-embedding-004" });
    const result = await model.embedContent(text);
    return result.embedding.values;
}

// ==========================================
// API SCAN CV
// ==========================================
app.post('/api/cv/upload', upload.single('cv_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu file CV' });
        
        console.log(`ü§ñ ƒêang x·ª≠ l√Ω file: ${req.file.originalname}`);

        const jobId = req.body.job_id;
        let jobCriteria = null;
        if (jobId) {
            const jobRes = await pool.query('SELECT * FROM job_positions WHERE id = $1', [jobId]);
            if (jobRes.rows.length > 0) jobCriteria = jobRes.rows[0];
        }

        // --- G·ªåI AI V·ªöI MODEL C·ªê ƒê·ªäNH ---
        // S·ª≠ d·ª•ng apiVersion: 'v1beta' ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch
        const model = genAI.getGenerativeModel({ model: MODEL_NAME }, { apiVersion: 'v1beta' });
        
        let prompt = `B·∫°n l√† chuy√™n gia HR. H√£y ph√¢n t√≠ch CV ƒë√≠nh k√®m.`;
        if (jobCriteria) {
            const reqs = jobCriteria.requirements;
            prompt += ` So s√°nh v·ªõi JD: ${jobCriteria.title}, K·ªπ nƒÉng: ${reqs.skills}. ƒê√°nh gi√° % ph√π h·ª£p.`;
        }

        prompt += ` Tr·∫£ v·ªÅ JSON duy nh·∫•t: { "full_name": "T√™n", "email": "Email", "skills": [], "score": 0-100, "match_reason": "L√Ω do", "summary": "T√≥m t·∫Øt" }`;

        const imageParts = [{
            inlineData: {
                data: req.file.buffer.toString("base64"),
                mimeType: req.file.mimetype,
            },
        }];

        const result = await model.generateContent([prompt, ...imageParts]);
        const responseText = result.response.text().replace(/```json|```/g, '').trim();
        const aiResult = JSON.parse(responseText);

        // L∆∞u DB
        const finalScore = aiResult.score > 10 ? (aiResult.score / 10).toFixed(1) : aiResult.score;
        const finalName = req.body.full_name || aiResult.full_name || "·ª®ng vi√™n M·ªõi";

        const dbResult = await pool.query(
            `INSERT INTO candidates (organization_id, job_id, full_name, email, role, status, ai_rating, ai_analysis) 
             VALUES (1, $1, $2, $3, $4, 'Screening', $5, $6) RETURNING *`,
            [jobId || null, finalName, aiResult.email, jobCriteria ? jobCriteria.title : '·ª®ng vi√™n t·ª± do', finalScore, JSON.stringify(aiResult)]
        );

        res.json({ message: "Th√†nh c√¥ng!", candidate: dbResult.rows[0] });

    } catch (err) { 
        console.error("L·ªói Server:", err);
        // M·∫πo: In ra l·ªói chi ti·∫øt ƒë·ªÉ bi·∫øt Key c√≥ v·∫•n ƒë·ªÅ kh√¥ng
        res.status(500).json({ error: "L·ªói AI: " + err.message + " (H√£y ki·ªÉm tra l·∫°i API Key t·ª´ AI Studio)" }); 
    }
});

// ... (Gi·ªØ nguy√™n c√°c API danh s√°ch, training, chat, import job c≈©) ...
app.get('/api/candidates', async (req, res) => {
    const result = await pool.query('SELECT * FROM candidates ORDER BY id DESC');
    res.json(result.rows);
});
app.get('/api/jobs', async (req, res) => {
    const result = await pool.query('SELECT * FROM job_positions ORDER BY id DESC');
    res.json(result.rows);
});
app.post('/api/jobs/import', upload.single('csv_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu CSV' });
        const results = [];
        const stream = require('stream').Readable.from(req.file.buffer);
        stream.pipe(csv()).on('data', (data) => results.push({
            title: data.Title || 'Job m·ªõi',
            requirements: { skills: data.Skills ? data.Skills.split('|') : [], experience: data.Experience || 0 },
            status: 'active'
        })).on('end', async () => {
            for (const job of results) await pool.query(`INSERT INTO job_positions (title, requirements, status) VALUES ($1, $2, 'active')`, [job.title, JSON.stringify(job.requirements)]);
            res.json({ message: "Import xong!" });
        });
    } catch (err) { res.status(500).json({ error: err.message }); }
});
app.post('/api/training/upload', upload.single('doc_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu file' });
        let rawText = "";
        if (req.file.mimetype === 'application/pdf') rawText = await readPdfBuffer(req.file.buffer);
        else if (req.file.mimetype.includes('word')) { const r = await mammoth.extractRawText({ buffer: req.file.buffer }); rawText = r.value; }
        const chunks = chunkText(rawText);
        for (const chunk of chunks) {
            if(!chunk.trim()) continue;
            const vector = await createEmbedding(chunk);
            await pool.query(`INSERT INTO documents (content, metadata, embedding) VALUES ($1, $2, $3)`, [chunk, JSON.stringify({ filename: req.file.originalname }), `[${vector.join(',')}]`]);
        }
        res.json({ message: "Training xong!" });
    } catch (err) { res.status(500).json({ error: err.message }); }
});
app.post('/api/training/chat', async (req, res) => {
    try {
        const { query } = req.body;
        const queryVector = await createEmbedding(query);
        const searchResult = await pool.query(`select content from match_documents($1, 0.5, 5)`, [`[${queryVector.join(',')}]`]);
        const context = searchResult.rows.map(r => r.content).join("\n---\n");
        const model = genAI.getGenerativeModel({ model: MODEL_NAME });
        const result = await model.generateContent(`D·ª±a v√†o: ${context} \nTr·∫£ l·ªùi: ${query}`);
        res.json({ answer: result.response.text() });
    } catch (err) { res.status(500).json({ error: err.message }); }
});

const PORT = process.env.PORT || 5000;
app.listen(PORT, () => {
    console.log(`Server ch·∫°y t·∫°i c·ªïng ${PORT}`);
});