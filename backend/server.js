/* FILE: backend/server.js (B·∫£n Full: AI + Storage) */
require('dotenv').config();
const express = require('express');
const cors = require('cors');
const { Pool } = require('pg');
const multer = require('multer');
const fs = require('fs'); 
const csv = require('csv-parser');
const mammoth = require('mammoth'); 
const pdf = require('pdf-parse'); 
const { GoogleGenerativeAI } = require("@google/generative-ai");
const { createClient } = require('@supabase/supabase-js'); // Th∆∞ vi·ªán Supabase

const app = express();
app.use(cors());
app.use(express.json());

// --- C·∫§U H√åNH ---
// Model AI ·ªïn ƒë·ªãnh nh·∫•t
let ACTIVE_MODEL_NAME = "gemini-2.5-flash"; 

// C·∫•u h√¨nh Memory Storage
const storage = multer.memoryStorage();
const upload = multer({ storage: storage });

// 1. K·∫øt n·ªëi Postgres DB
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: { rejectUnauthorized: false }
});

// 2. K·∫øt n·ªëi AI Gemini
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// 3. K·∫øt n·ªëi Supabase Storage (QUAN TR·ªåNG ƒê·ªÇ L∆ØU FILE)
const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_KEY);

// --- C√ÅC H√ÄM H·ªñ TR·ª¢ ---

// H√†m l√†m s·∫°ch JSON (Tr√°nh l·ªói c√∫ ph√°p t·ª´ AI)
function cleanJsonString(text) {
    let clean = text.replace(/```json/g, '').replace(/```/g, '').trim();
    const firstOpen = clean.indexOf('{');
    const lastClose = clean.lastIndexOf('}');
    if (firstOpen !== -1 && lastClose !== -1) {
        clean = clean.substring(firstOpen, lastClose + 1);
    }
    return clean;
}

async function readPdfBuffer(buffer) {
    try { return (await pdf(buffer)).text; } catch (e) { return ""; }
}

function chunkText(text, chunkSize = 1000) {
    const chunks = []; let cur = ""; 
    text.split(/(?<=[.?!])\s+/).forEach(s => {
        if ((cur + s).length > 1000) { chunks.push(cur); cur = s; } else cur += " " + s;
    });
    if (cur) chunks.push(cur);
    return chunks;
}

async function createEmbedding(text) {
    const model = genAI.getGenerativeModel({model: ACTIVE_MODEL_NAME});
    const result = await model.embedContent(text);
    return result.embedding.values;
}

// ==========================================
// API 1: SCAN CV & UPLOAD FILE
// ==========================================
app.post('/api/cv/upload', upload.single('cv_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu file CV' });
        console.log(`ü§ñ ƒêang x·ª≠ l√Ω: ${req.file.originalname}`);

        // --- PH·∫¶N 1: UPLOAD FILE L√äN SUPABASE STORAGE ---
        // T·∫°o t√™n file kh√¥ng tr√πng (d√πng th·ªùi gian hi·ªán t·∫°i)
        const fileName = `${Date.now()}-${req.file.originalname.replace(/\s+/g, '_')}`;
        
        // Upload l√™n Bucket 'cv_uploads'
        const { data: uploadData, error: uploadError } = await supabase
            .storage
            .from('cv_uploads')
            .upload(fileName, req.file.buffer, {
                contentType: req.file.mimetype,
                upsert: false
            });

        if (uploadError) {
            console.error("L·ªói Storage:", uploadError);
            // N·∫øu l·ªói upload file th√¨ v·∫´n cho qua ƒë·ªÉ AI ch·∫°y ti·∫øp, nh∆∞ng kh√¥ng c√≥ link xem l·∫°i
        }

        // L·∫•y link c√¥ng khai (Public URL)
        const { data: { publicUrl } } = supabase.storage.from('cv_uploads').getPublicUrl(fileName);
        const finalFileUrl = uploadError ? null : publicUrl;
        console.log("üåç File URL:", finalFileUrl);


        // --- PH·∫¶N 2: X·ª¨ L√ù AI (JSON MODE) ---
        const jobId = req.body.job_id;
        let jobCriteria = null;
        if (jobId) {
            const jobRes = await pool.query('SELECT * FROM job_positions WHERE id = $1', [jobId]);
            if (jobRes.rows.length > 0) jobCriteria = jobRes.rows[0];
        }

        const model = genAI.getGenerativeModel({ 
            model: ACTIVE_MODEL_NAME,
            generationConfig: { responseMimeType: "application/json" }
        });
        
        let prompt = `B·∫°n l√† chuy√™n gia HR. Tr√≠ch xu·∫•t th√¥ng tin t·ª´ CV ƒë√≠nh k√®m.`;
        if (jobCriteria) {
            prompt += ` So s√°nh v·ªõi JD: ${jobCriteria.title}, K·ªπ nƒÉng: ${JSON.stringify(jobCriteria.requirements)}.`;
        }

        prompt += `
        Output JSON format:
        {
            "full_name": "T√™n",
            "email": "Email",
            "skills": ["Skill1", "Skill2"],
            "score": 8.5,
            "summary": "T√≥m t·∫Øt",
            "match_reason": "L√Ω do ƒëi·ªÉm s·ªë"
        }
        `;

        const imageParts = [{
            inlineData: {
                data: req.file.buffer.toString("base64"),
                mimeType: req.file.mimetype,
            },
        }];

        const result = await model.generateContent([prompt, ...imageParts]);
        const responseText = result.response.text();
        
        let aiResult;
        try {
            aiResult = JSON.parse(cleanJsonString(responseText));
        } catch (parseError) {
            aiResult = { full_name: "·ª®ng vi√™n", score: 0, summary: "L·ªói ƒë·ªçc AI", email: null };
        }

        const finalName = req.body.full_name || aiResult.full_name || "·ª®ng vi√™n M·ªõi";
        const finalScore = aiResult.score > 10 ? (aiResult.score / 10).toFixed(1) : aiResult.score;

        // --- PH·∫¶N 3: L∆ØU DATABASE (K√àM LINK FILE) ---
        const dbResult = await pool.query(
            `INSERT INTO candidates (organization_id, job_id, full_name, email, role, status, ai_rating, ai_analysis, cv_file_url) 
             VALUES (1, $1, $2, $3, $4, 'Screening', $5, $6, $7) RETURNING *`,
            [
                jobId || null,
                finalName, 
                aiResult.email, 
                jobCriteria ? jobCriteria.title : '·ª®ng vi√™n t·ª± do', 
                finalScore, 
                JSON.stringify(aiResult),
                finalFileUrl // <--- QUAN TR·ªåNG: L∆∞u link file v√†o ƒë√¢y
            ]
        );

        res.json({ message: "Th√†nh c√¥ng!", candidate: dbResult.rows[0] });

    } catch (err) { 
        console.error("üî• L·ªói Server:", err);
        res.status(500).json({ error: "L·ªói: " + err.message }); 
    }
});

// ... (GI·ªÆ NGUY√äN C√ÅC API KH√ÅC: list candidates, jobs, training, chat...) ...
app.get('/api/candidates', async (req, res) => {
    const result = await pool.query('SELECT * FROM candidates ORDER BY id DESC');
    res.json(result.rows);
});
app.get('/api/jobs', async (req, res) => {
    const result = await pool.query('SELECT * FROM job_positions ORDER BY id DESC');
    res.json(result.rows);
});
app.post('/api/jobs/import', upload.single('csv_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu CSV' });
        const results = [];
        const stream = require('stream').Readable.from(req.file.buffer);
        stream.pipe(csv()).on('data', (data) => results.push({
            title: data.Title || 'Job m·ªõi',
            requirements: { skills: data.Skills ? data.Skills.split('|') : [], experience: data.Experience || 0 },
            status: 'active'
        })).on('end', async () => {
            for (const job of results) await pool.query(`INSERT INTO job_positions (title, requirements, status) VALUES ($1, $2, 'active')`, [job.title, JSON.stringify(job.requirements)]);
            res.json({ message: "Import Done" });
        });
    } catch (err) { res.status(500).json({ error: err.message }); }
});
app.post('/api/training/upload', upload.single('doc_file'), async (req, res) => {
    try {
        if (!req.file) return res.status(400).json({ error: 'Thi·∫øu file' });
        let rawText = "";
        if (req.file.mimetype === 'application/pdf') rawText = await readPdfBuffer(req.file.buffer);
        else if (req.file.mimetype.includes('word')) { const r = await mammoth.extractRawText({ buffer: req.file.buffer }); rawText = r.value; }
        const chunks = chunkText(rawText);
        for (const chunk of chunks) {
            const vector = await createEmbedding(chunk);
            await pool.query(`INSERT INTO documents (content, metadata, embedding) VALUES ($1, $2, $3)`, [chunk, JSON.stringify({ filename: req.file.originalname }), `[${vector.join(',')}]`]);
        }
        res.json({ message: "Training Done" });
    } catch (err) { res.status(500).json({ error: err.message }); }
});
app.post('/api/training/chat', async (req, res) => {
    try {
        const { query } = req.body;
        const queryVector = await createEmbedding(query);
        const searchResult = await pool.query(`select content from match_documents($1, 0.5, 5)`, [`[${queryVector.join(',')}]`]);
        const context = searchResult.rows.map(r => r.content).join("\n---\n");
        const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
        const result = await model.generateContent(`Context: ${context} \nAnswer: ${query}`);
        res.json({ answer: result.response.text() });
    } catch (err) { res.status(500).json({ error: err.message }); }
});

const PORT = process.env.PORT || 5000;
app.listen(PORT, () => {
    console.log(`Server ch·∫°y t·∫°i c·ªïng ${PORT}`);
});